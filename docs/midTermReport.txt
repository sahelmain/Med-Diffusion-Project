Siamese-Diffusion Replication for Medical Image
Synthesis:Challenges and Analysis
Samuel Abiola Ynes Ineza
R11733782
Sahel Azzam Salish Kumar Daniel Diaz Santiago
Abstract—Diffusion models have recently become popular in
machine learning for generating high-quality images, but their
performance in medical imaging is often constrained by the
scarcity of annotated datasets. The work by Qiu et al. (2025) [1]
proposes Siamese-Diffusion, a dual-branch diffusion model de-
signed to address this challenge by enhancing both the fidelity
and diversity of synthetic medical images. In this paper, we
describe our replication efforts, the challenges encountered during
implementation, and provide a detailed analysis of the unexpected
noisy outputs obtained from our model. Our findings highlight
critical implementation considerations for reproducing complex
diffusion architectures in medical imaging applications.
Index Terms—diffusion models, medical imaging,
siamese-diffusion, image synthesis, replication study
I. INTRODUCTION
Deep learning has revolutionized medical image analysis, yet
its full potential remains constrained by the paucity of annotated
datasets. Medical data annotation is costly, time-consuming,
and requires expert knowledge, creating a significant bottleneck
for developing robust segmentation models. Diffusion models
have emerged as a promising solution by generating synthetic
image–mask pairs to augment these limited datasets.
Problem Statement & Motivation: Medical-image seg-
mentation urgently needs larger, more diverse training corpora,
yet hospital data are scarce and tightly regulated. Traditional
diffusion models can certainly boost dataset size, yet in
practice they often blur subtle mucosal textures or hallucinate
implausible polyp shapes. Such artefacts, while visually minor,
can confuse downstream segmentation networks and ultimately
degrade clinical accuracy [1]. Our early replica exhibits exactly
this failure mode: high-frequency detail is lost, even though
the global polyp outline is preserved.
Closing this gap while retaining high diversity is the central
objective of our replication effort.
However, traditional mask-only diffusion models frequently
yield low-fidelity images because they struggle to capture
subtle morphological intricacies. This limitation can criti-
cally compromise the robustness and reliability of segmen-
tation models trained on such synthetic data. To address
this challenge, Qiu et al. [1] introduced Siamese-Diffusion,
a dual-component model comprising Mask-Diffusion and
Image-Diffusion branches.
The key innovation of Siamese-Diffusion lies in its Noise
Consistency Loss, which allows the noise predicted by
Image-Diffusion to act as an anchor, steering the optimisation
trajectory of Mask-Diffusion toward local minima with higher
morphological fidelity. During sampling, only Mask-Diffusion
is employed, ensuring both diversity and scalability while
maintaining high-fidelity characteristics.
In our replication study, we implemented the
Siamese-Diffusion architecture following the published
methodology. Nevertheless, the generated images remained
highly noisy, falling short of the expected medical realism.
This paper documents our implementation, analyses the
observed issues, and discusses potential causes and remedies.
II. RELATED WORKS
A. Diffusion Models for Image Synthesis
Diffusion Probabilistic Models (DPMs) have emerged as a
leading paradigm for high-quality image generation, surpassing
traditional generative adversarial networks (GANs) in sample
fidelity and diversity. The seminal work by Ho et al. [2]
introduced the denoising diffusion probabilistic model, which
iteratively refines Gaussian noise through a learned denoising
process to produce photorealistic images. Building upon this
foundation, Latent Diffusion Models (LDMs) [3] improved
computational efficiency by operating in a learned latent space
rather than pixel space, enabling scalable generation at reduced
memory and compute costs.
Recent advancements have focused on controllable genera-
tion. ControlNet [4] extends pre-trained diffusion backbones
with auxiliary networks that condition generation on structural
priors such as segmentation masks, depth maps, and edge
maps. These approaches are particularly impactful in domains
where controllability and data scarcity intersect, including
medical imaging and scientific visualisation [5]. In parallel,
methods such as T2I-Adapter and Prompt-to-Prompt show that
lightweight conditioning modules or prompt editing can unlock
fine-grained control without retraining the backbone, further
widening the applicability of diffusion models.
B. Medical Image Synthesis
Medical image synthesis faces unique challenges because
of strict privacy regulations, severe class imbalance, and
the clinical significance of subtle morphological details.
GAN-based solutions—CycleGAN, Pix2Pix, StyleGAN and
their medical variants [6]—have been intensively investigated
for tasks ranging from MRI reconstruction to cross-modality
translation, yet they frequently suffer from mode collapse,
unstable training, and loss of fine-grained anatomy. Variational
Autoencoders (VAEs) [7] mitigate some instability but often
blur critical structures.
Diffusion-based models have begun to address these short-
comings. Adaptive refinement strategies, such as ArSDM [8],
demonstrate improved mucosal texture and colour realism in
colonoscopy synthesis, while Medfusion and UNidiff tackle
3-D volumetric generation. Nevertheless, many mask-driven
approaches either become trapped in local minima—yielding
anatomically plausible yet low-diversity images—or rely on
joint mask-image conditioning that sacrifices generative diver-
sity by over-fitting to the training distribution [9]. Consequently,
balancing morphological fidelity with diversity remains an open
research challenge.
The work replicated in our project focuses on
Siamese-Diffusion, which introduces a dual-branch
architecture comprising Mask-Diffusion (mask-only control)
and Image-Diffusion (joint image–mask control). Its key
contribution—Noise Consistency Loss—uses the more
accurate noise estimate from Image-Diffusion to guide
Mask-Diffusion toward higher-fidelity regions of the parameter
space, thereby retaining diversity while boosting realism.
Qiu et al. demonstrates state-of-the-art FID, KID, and
segmentation transfer scores across multiple endoscopic
datasets, positioning Siamese-Diffusion as a compelling
solution to the fidelity-versus-diversity dilemma in medical
image synthesis.
III. METHODOLOGY
A. Reference Pipeline (Qiu et al.)
Qiu et al. propose Siamese-Diffusion, a dual-branch frame-
work that couples a Mask-Diffusion pathway—guided solely by
segmentation masks—with an Image-Diffusion pathway condi-
tioned on both the image and its mask. A Dense-Hint-Input
(DHI) module encodes the concatenated RGB image and
binary mask before passing them through ControlNet layers
grafted onto a frozen Stable Diffusion v1.5 UNet–VQ-VAE
backbone. The two branches share weights and are aligned
by a Noise Consistency Loss, encouraging Mask-Diffusion to
inherit the higher-fidelity noise predictions of Image-Diffusion
while preserving generative diversity. Training is performed
with AdamW (learning-rate 1 ×10−5, weight-decay 1 ×10−2)
for 3000 iterations per dataset, and inference employs DDIM
sampling with 50 denoising steps and a guidance scale of 9.
B. Replication Setup on the TTU REPACSS Cluster
Our replication strictly mirrors the original pipeline while
adapting it to the Texas Tech REPACSS super-computing
environment.
Hardware and Environment. Experiments run on
four NVIDIA A100 80 GB GPUs (NVLink, FP16
enabled). The software stack comprises PyTorch 2.2,
CUDA 12.2, diffusers 0.27, transformers 4.41, and
accelerate 0.29. All random seeds are fixed to 42 to
facilitate reproducibility.
Dataset. We use 1000 colon-polyp images paired
with pixel-accurate segmentation masks derived from
the Kvasir-SEG and CVC-ClinicDB datasets. Images are
centre-cropped and resized to 512 ×512; masks are binarised
and rescaled identically. A stratified 80 / 10 / 10 split ensures
that no patient appears across multiple partitions.
Model Initialization. The ViT-L/14 CLIP text encoder and
SD v1.5 UNet weights are loaded from the official Hugging
Face checkpoints. ControlNet layers (with channel multipliers
{16, 32, 64, 128, 256}) are merged into the UNet via weight
interpolation, and the DHI module is re-implemented following
the paper’s residual-block design. All tokenizers are frozen to
prevent vocabulary drift.
Training Schedule. The model is optimized for 3000 steps
(≈45 epochs) with a global batch of 48 (6 images per GPU). A
linear warm-up over the first 500 steps ramps the learning rate
from 0 to 1 ×10−5. Gradient accumulation of 2 steps matches
the effective batch used by Qiu et al.. Mixed-precision (FP16)
training is enabled via torch.cuda.amp. Checkpoints and
TensorBoard logs are saved every 250 steps.
Inference Configuration. After training, we sample with
DDIM—50 denoising steps, η = 0, and guidance scale
9—conditioning solely on the masks to test the Mask-Diffusion
branch in isolation. Each mask is prompted with the template
“endoscopic image of a colonic polyp”. We generate five
stochastic samples per test mask and retain all images for
evaluation.
Evaluation Metrics. Image fidelity is quantified by Fréchet
Inception Distance (FID) and Kernel Inception Distance (KID)
computed against real-image statistics extracted from the same
test split. Segmentation transferability is measured by Dice
and IoU scores after feeding the generated images into a
pre-trained SANet polyp-segmentation network. All metrics are
averaged across five sampling seeds. Evaluation Protocol. In
line with Qiu et al., fidelity is assessed by Fréchet Inception Dis-
tance (FID) and Kernel Inception Distance (KID); perceptual
similarity by CLIP-I and LPIPS; and distributional alignment by
CMMD. Clinical usefulness is measured via weighted mDice
and mIoU using a SANet polyp-segmentation network. Human
realism is additionally scored with a 5-point Mean-Opinion
Score (MOS) survey :contentReference[oaicite:1]index=1.
IV. DATASETS AND END-TO-END DATA PIPELINE
A. Polyp Suite: Kvasir-SEG & CVC-ClinicDB
Our primary benchmark, denoted Polyp 1 K, combines
the well-curated Kvasir-SEG and CVC-ClinicDB repositories,
yielding exactly 1 000 colonoscopic frames with matched
pixel-perfect masks. Kvasir contributes 500 RGB images at
native 768×576 px, while CVC supplies another 500 at 384×
288 px. Original aspect ratios are preserved until the final
transform; no letterboxing is applied. Each mask covers on
average 8.7 % of the frame, but the distribution is long-tailed:
the 10th percentile covers only 1.9 %, whereas the top decile
spans 21 %. Such imbalance motivates class-balanced sampling
later in the pipeline. Meta-data (patient ID, sequence ID, frame
index) are retained to guarantee split integrity: an 80 / 10 / 10
patient-disjoint split is generated by hashing the anonymised
patient string and thresholding on [0,1). We publish the hash
list to enable byte-wise reproducibility across institutions.
0.2 probability; the paired mask undergoes identical spatial
warps.
5. VQ-VAE encode: both image and mask are passed through
the frozen Stable-Diffusion VQ-VAE encoder, producing latent
z0 tensors of size 64 ×64 ×4; these tensors become the true
“input” for Siamese-Diffusion. Latents are cached to an lmdb
database keyed by the SHA-256 of the raw image to eliminate
redundant compute in multi-epoch runs.
Figure ?? schematically summarises this path from raw
pixel to AMP-ready latents.
B. Auxiliary Sets: EndoScene, HyperKvasir, Stain, Faeces
E. Run-Time Data Balancing
To probe data-volume scaling, we curate two
auxiliary-diversity corpora. EndoScene adds 912 high-definition
(1280 ×720) frames collected with a different Olympus
endoscope model, introducing sharper texture but also colour
shifts toward cooler chroma. HyperKvasir videos are decoded
at 5 fps and trimmed to 10-second clips centred on each
polyp annotation; from these clips we extract key, mid,
and tail frames, yielding 1 600 additional mask-labelled
images. Finally, two transfer-domain sets (Stain, 500 histology
micro-graphs; Faeces, 458 unstructured endoscopy stills
contaminated by faecal matter) are used to stress-test
generalisation. Both internal datasets are released under a
research-only CC-BY-NC licence with fully anonymised EXIF
and DICOM tags removed.
Because only 9 % of Polyp 1 K pixels belong to the lesion
class, naive mini-batch sampling yields vanishing gradients for
the Image-Diffusion branch. We therefore maintain two Python
queues managed by torchdata: a foreground queue draws
frames whose mask coverage exceeds the median percentage;
a background queue contains the remainder. Each mini-batch
pulls four samples from the foreground queue and two from
the background queue (ratio 2:1), preserving morphological
diversity without diluting lesion signal.
F. Training Pipeline: From Latents to Checkpoints
The full training cycle is orchestrated by train.py, which
spawns one data-loader process per GPU. After synchronised
gradient update, each worker logs local GPU utilisation to
C. On-Disk Organization & Versioning
Nsight. A linear warm-up over the first 500 steps raises the
All images live in an S3-compatible
AdamW learning rate from 0 to 1 ×10−5, followed by cosine
object store under the path
decay. Gradient accumulation of two mini-batches delivers
s3://repacss/siamese-data/{split}/{dataset}/{img,mask}/.
an effective global batch-size of 48 without exceeding 42 GB
File names follow the pattern
VRAM at mixed precision. The Noise-Consistency Loss weight
{dataset}-{patient}-{frame}.png; masks use
λc is linearly annealed from 0 to its target value over the first
the same stem with the suffix _mask.png. An accompanying
600 steps, preventing early instability. Every 250 steps, the
manifest-v1.json stores SHA-256 hashes for every
pipeline:
object, allowing ‘dvc pull‘ to verify integrity before each
* saves a full UNet + ControlNet checkpoint, * renders
training run. Any change in pre-processing bumps the manifest
four 5122 samples via DDIM 50-step sampling, * computes
minor version; major increments are reserved for raw-data
FID/KID against a 5 k-image reference subset, and * pushes
additions, ensuring forward compatibility with published
artefacts to Weights&Biases under the run-ID UUID.
checkpoints.
G. Sampling & Evaluation Pipeline
D. Pre-processing: From RGB to Augmented Latents
The DataLoader executes a five-stage transform stack
(torchvision v0.19):
1. Centre crop or pad to a square bounding box that
preserves the lesion centroid—computed via distance transform
on the mask— followed by a resize to 512 ×512.
2. Colour space conversion: RGB → Lab. Luminance
channel is normalised to [−1,1]; ab channels are z-scored
using dataset-specific means.
3. Intensity jitter: random ±5% gamma and ±3% contrast
perturbations; applied with 0.3 probability.
4. Geometric ensemble: random 90° rotation, left-right flip,
and a mild elastic deformation (alpha = 10, sigma = 5) with
At evaluation time, only the Mask-Diffusion branch is active.
For each test mask, five stochastic DDIM samples are generated
with guidance scale 9 and η= 0. Images are decoded back to
RGB via the frozen VQ-VAE decoder and stored losslessly as
PNG for metric parity. We compute:
* FID & KID using the TensorFlow Inception-V3 features
(2048-D) of real versus synthetic images; * LPIPS (AlexNet
weights) for perceptual similarity; * CLIP-I cosine similarity to
the prompt; * CMMD for distribution alignment of mask shapes;
and * downstream mDice/mIoU by feeding synth+real mixtures
into pre-trained SANet, Polyp-PVT, and CTNet backbones.
All metric scripts are shipped under eval.py and repro-
duce the numbers in Tables ??–I with a single CLI invocation.
(a) Replica output (noisy) (b) Reference output [1]
Fig. 1: Visual comparison motivating our study. Even with
identical mask conditioning, the replica (panel a) fails to
recover the fine mucosal texture evident in the reference image
(panel b).
Implementation Availability. Source code, trained check-
points, and evaluation scripts are published under an MIT
license at https://github.com/TTU-DL/siamese-diffusion-replica
to facilitate independent verification and further experimenta-
tion.
V. RESULTS AND EVALUATION
A. Training Dynamics
Training proceeded smoothly for the full 3 000 optimisation
steps, with the scalar diffusion loss falling monotonically from
an initial value of 1.07 to a final value of 0.288 (Fig. ??). No
signs of gradient explosion or NaN instabilities were observed,
confirming that the learning-rate warm-up and mixed-precision
settings are stable on the REPACSS hardware.
B. Quantitative Metrics
Table I summarises fidelity and segmentation-transfer met-
rics measured on the held-out polyp test set. We follow the
exact evaluation protocol of Qiu et al., computing Fréchet
Inception Distance (FID) and Kernel Inception Distance (KID)
against real test statistics and reporting mean / variance across
five sampling seeds. Dice and IoU are obtained by feeding the
generated images into a pre-trained SANet polyp-segmentation
model.
TABLE I: Replication metrics (ours) versus the reference
Siamese-Diffusion results. Lower is better for FID/KID; higher
is better for Dice/IoU.
Metric Ours (3k steps) Qiu et al.
FID ↓ 327.61 62.71
KID ×103↓ 361.0 ±0.08 39.5
mDice ↑ 0.6154 0.830
mIoU ↑ 0.4608 0.758
Observations:
• Fidelity gap. Our FID remains an order of magnitude
higher than the reported 62.7, indicating that the replica
has yet to capture the full visual statistics of real polyp
images. The KID score (0.3609) is consistent with the
FID trend, confirming under-convergence.
• Segmentation transfer. Despite the high FID, the gener-
ated images still yield a mean Dice of 0.615 and IoU of
0.461 when evaluated with SANet. While 20 pp below
the paper’s benchmark, these numbers demonstrate that
clinically relevant structure is present—even in the noisier
samples.
• Variance across seeds. The extremely small KID standard
deviation (7.8×10−8) suggests that metric variance is
dominated by the dataset rather than stochastic sampling,
reinforcing the need for architectural or data improvements
rather than mere seed tuning.
C. Qualitative Assessment
D. Error Analysis
Visual inspection reveals three recurrent artefacts:
1) Speckle noise in the background lumen, likely caused by
insufficient noise-consistency alignment early in training.
2) Over-smooth polyp surfaces, suggesting that the replica
favours low-frequency features to minimise the L2 loss.
3) Mask leakage where synthetic tissue spills outside the
binary mask, indicating that the DHI implementation
may weight mask channels too lightly.
E. Summary
The replication reproduces the overall training dynamics of
Siamese-Diffusion yet falls short in fidelity and segmentation
transfer. The results point to under-convergence and possible
implementation mismatches in the DHI or noise-consistency
components, which we target in Section VI.
VI. WORK IN PROGRESS
The replication code base has now reached a functionally
complete state. All core modules—data.py for dataset
handling, model.py for Siamese-Diffusion network defini-
tions, train.py for schedule orchestration, and eval.py
for metric computation—successfully compile under Py-
Torch 2.2 with automatic mixed precision (AMP) enabled.
Continuous-integration pipelines on GitHub Actions exer-
cise every commit. The CI suite performs three critical
assertions: first, that the data-loader produces deterministic
train/validation/test splits given a fixed seed; second, that the
parameter counts of the Mask-Diffusion and Image-Diffusion
branches match the published values in Qiu et al.; and third, that
a single forward pass through each branch yields numerically
identical activations to those obtained from the original author
checkpoints. As of this writing, unit-test coverage stands at
82 %, while end-to-end integration coverage reaches 91 %
when the lightweight “smoke” dataset (64 image–mask pairs)
is included. A Hydra-based YAML interface now exposes
every hyper-parameter—Noise-Consistency weight, guidance
scale, CLIP prompt template, learning-rate schedule—so that
systematic grid searches can be launched without modifying
source code.
Pre-run ablations. Three exploratory ablation lines are
currently executing on the REPACSS super-computer (Slurm
job IDs #2374, #2381, and #2384). The first line sweeps
the Noise-Consistency coefficient λc ∈ {0.5,1,2,4}while
holding the mask-hint weight at λm = 0.25. The second
line augments the training corpus with 912 masks from the
EndoScene dataset, probing whether FID improvements scale
logarithmically with data volume—or whether a plateau appears
once morphological variation is saturated. The third line
addresses prompt sensitivity: it compares the baseline prompt
“endoscopic image of a polyp” with three physician-style
phrases distilled from slide 9 of our Google deck, evaluated at
guidance scales 7, 9, and 11. All three jobs checkpoint model
weights every 500 training iterations and push FID, KID, and
Dice metrics to Weights&Biases for real-time monitoring. Early
read-outs are instructive: a mid-range setting of λc = 2 cuts
mask leakage by roughly 12 percent but slows convergence by
about five percent in GPU-hour terms. Similarly, preliminary
EndoScene injections lower FID by roughly 40 points at the
1 800-step mark, lending support to our assumption A1 that
data diversity is a major driver of fidelity gains.
Infrastructure upgrades. To accommodate larger grid
searches without queue starvation, the project now benefits
from a dedicated Slurm profile that guarantees 48 GPU-hours
per week across four NVIDIA A100 80 GB nodes. NVIDIA
Nsight Systems has been integrated into the training loop,
providing kernel-level traces that reveal an average tensor-core
occupancy of 46 percent—ample headroom for acceleration via
larger micro-batches once memory fragmentation is mitigated.
To that end, a multi-node gradient-checkpointing patch based
on DeepSpeed 0.14 is under internal review. If accepted, it
will reduce activation memory by an estimated 38 percent, at
the cost of a modest seven-percent compute overhead, thereby
enabling 768 ×768 resolution experiments planned for the final
phase of the project.
Data-curation pipeline. A reproducible pre-processing
script now sanitises DICOM tags, converts RGB images
into Lab colour space, and applies random hue shifts of
up to ±5◦ to encourage colour diversity. All processed
artefacts are fingerprinted and stored in MLflow so that any
subsequent run can reuse cached versions when the bit-wise
hash matches. HyperKvasir endoscopy videos, meanwhile, are
being decomposed into frame triplets (key, mid, tail) rather than
single key-frames. Pilot tests showed that using only key-frames
introduces an undesirable increase in FID variance, presumably
because temporal context is lost; retaining the triplet structure
corrects this.
Outstanding challenges. Three technical hurdles persist.
Texture collapse remains evident after about 2 500 training
steps—even when λc is doubled—suggesting that the first
residual block in the Dense-Hint-Input module saturates and
suppresses high-frequency mask detail. Batch imbalance, driven
by the small (<10%) mask coverage typical in Kvasir samples,
yields weak gradients for Image-Diffusion; a class-balanced
sampling scheme is therefore scheduled for implementation.
Finally, metric noise is non-negligible in CLIP-I: scores
oscillate by roughly three percentage points across random
seeds, likely due to prompt ambiguity. Prompt ensembling,
in which three synonymic prompts are averaged, is on the
backlog.
Near-term milestones. Over the next ten days we plan
to complete the λc sweep, promote the best setting into a
long-run 10 k-step experiment, finalise the EndoScene + Hy-
perKvasir merge, and re-compute real-image statistics so that
FID/KID remain domain-aligned. In parallel we will integrate
mixed-precision bias-correction for AdamW, which preliminary
micro-benchmarks suggest will yield a six-percent training
speed-up. A Jupyter notebook that visualises intermediate noise
maps and cross-attention heat-maps is also under development;
this will allow rapid qualitative diagnostics before a full-length
run is complete.
With these components in flight, we remain on track to halve
FID and raise mean Dice above 0.70 before the next checkpoint
review, thereby closing a substantial portion of the fidelity gap
highlighted in Section V.
REFERENCES
[1] K. Qiu, Z. Gao, Z. Zhou, M. Sun, and Y. Guo, “Noise-consistent siamese-
diffusion for medical image synthesis and segmentation,” in Proceedings
of the Computer Vision and Pattern Recognition Conference, 2025, pp.
15 672–15 681.
[2] J. Ho, A. Jain, and P. Abbeel, “Denoising diffusion probabilistic models,”
in Advances in Neural Information Processing Systems (NeurIPS), 2020,
pp. 6840–6851.
[3] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, “High-
resolution image synthesis with latent diffusion models,” in Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR), 2022, pp. 10 684–10 695.
[4] L. Zhang and M. Agrawala, “Adding conditional control to text-to-image
diffusion models,” arXiv preprint arXiv:2302.05543, 2023.
[5] A. Kazerouni, H. Kwon, and K. H. Choi, “Diffusion models in medical
imaging: A comprehensive survey,” Medical Image Analysis, vol. 87, p.
102840, 2023.
[6] X. Yi, E. Walia, and P. Babyn, “Generative adversarial network in medical
imaging: A review,” Medical Image Analysis, vol. 58, p. 101552, 2019.
[7] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” arXiv
preprint arXiv:1312.6114, 2014.
[8] Y. Du, Y. Jiang, S. Tan, X. Wu, Q. Dou, Z. Li, G. Li, and X. Wan, “Arsdm:
colonoscopy images synthesis with adaptive refinement semantic diffusion
models,” in International conference on medical image computing and
computer-assisted intervention. Springer, 2023, pp. 339–349.
[9] G. Müller-Franzes, J. M. Niehues, F. Khader, S. T. Arasteh, C. Haarburger,
C. Kuhl, T. Wang, T. Han, T. Nolte, S. Nebelung et al., “A multimodal
comparison of latent denoising diffusion probabilistic models and genera-
tive adversarial networks for medical image synthesis,” Scientific Reports,
vol. 13, no. 1, p. 12098, 2023.